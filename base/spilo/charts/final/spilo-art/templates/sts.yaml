apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: &cluster_name {{ include "spilo-art.fullname" . }}
  labels:
    {{- include "spilo-art.labels" . | nindent 4 }}
  {{- if .Values.annotations }}
  annotations:
    {{- toYaml .Values.annotations | nindent 4 }}
  {{- end }}
spec:
  selector:
    matchLabels:
      {{- include "spilo-art.selectorLabels" . | nindent 6 }}
  replicas: {{ .Values.replicas }}
  serviceName: *cluster_name
  podManagementPolicy: {{ .Values.podManagementPolicy }}
  template:
    metadata:
      labels:
        {{- include "spilo-art.selectorLabels" . | nindent 8 }}
      {{- if .Values.podAnnotations }}
      annotations:
        {{- toYaml .Values.podAnnotations | nindent 8 }}
      {{- end }}
    spec:
      # service account that allows changing endpoints and assigning pod labels
      # in the given namespace: https://kubernetes.io/docs/user-guide/service-accounts/
      # not required unless you've changed the default service account in the namespace
      # used to deploy Spilo
      serviceAccountName: {{ include "spilo-art.serviceAccountName" . }}
      containers:
      - name: {{ .Chart.Name }}
        image: {{ .Values.image.name }}:{{ .Values.image.tag }}  # put the spilo image here
        imagePullPolicy: {{ .Values.image.imagePullSecrets }}
        ports:
        - containerPort: 8008
          protocol: TCP
        - containerPort: 5432
          protocol: TCP
        volumeMounts:
        {{- if .Values.backup.enable }}
        - mountPath: /data/pg_wal
          name: backup
        - mountPath: /config
          name: config
        {{- end }}
        - mountPath: /home/postgres/pgdata
          name: pgdata
        {{- if .Values.probes }}
        {{- toYaml .Values.probes | nindent 8 }}
        {{- end }}
        {{- if .Values.resources }}
        resources:
          {{- toYaml .Values.resources | nindent 10 }}
        {{- end }}
        env:
        - name: DCS_ENABLE_KUBERNETES_API
          value: 'true'
#        - name: ETCD_HOST
#          value: 'test-etcd.default.svc.cluster.local:2379' # where is your etcd?
#        - name: WAL_S3_BUCKET
#          value: example-spilo-dbaas
#        - name: LOG_S3_BUCKET # may be the same as WAL_S3_BUCKET
#          value: example-spilo-dbaas
#        - name: BACKUP_SCHEDULE
#          value: "00 01 * * *"
        - name: KUBERNETES_SCOPE_LABEL
          value: {{ .Values.spilo.env.kubernetesScopeLabel }}
        - name: KUBERNETES_ROLE_LABEL
          value: {{ .Values.spilo.env.kubernetesRoleLabel }}
        - name: KUBERNETES_LABELS
          value: '{{ toJson .Values.spilo.env.kubernetesLabels }}'
        - name: SPILO_CONFIGURATION
          value: | ## https://github.com/zalando/patroni#yaml-configuration
            {{- .Values.spilo.env.configuration | nindent 12 }}
        - name: POD_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.podIP
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        - name: PGPASSWORD_SUPERUSER
          valueFrom:
            secretKeyRef:
              name: {{ default "*cluster_name" .Values.secret.externalSecretName }}
              key: superuser-password
        - name: PGUSER_ADMIN
          value: superadmin
        - name: PGPASSWORD_ADMIN
          valueFrom:
            secretKeyRef:
              name: {{ default "*cluster_name" .Values.secret.externalSecretName }}
              key: admin-password
        - name: PGPASSWORD_STANDBY
          valueFrom:
            secretKeyRef:
              name: {{ default "*cluster_name" .Values.secret.externalSecretName }}
              key: replication-password
        - name: SCOPE
          value: *cluster_name
        - name: PGROOT
          value: /home/postgres/pgdata/pgroot
        {{- if .Values.backup.enable }}
        - name: WALG_FILE_PREFIX
          value: "/data/pg_wal"
        - name: CRONTAB
          value: "[\"{{ .Values.backup.crontabTime }} envdir /config /scripts/postgres_backup.sh /home/postgres/pgdata/pgroot/data\"]"
        {{- end }}
      terminationGracePeriodSeconds: 0
      {{- if .Values.nodeAffinity }}
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              {{- toYaml .Values.nodeAffinity.nodeSelectorTerms | nindent 14 }}
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: {{ .Values.spilo.env.kubernetesScopeLabel }}
                    operator: In
                    values:
                      - *cluster_name
              topologyKey: "kubernetes.io/hostname"
      {{- end }}
      {{- if .Values.tolerations }}
      tolerations:
        {{- toYaml .Values.tolerations | nindent 8 }}
      {{- end }}
      {{- if .Values.backup.enable }}
      volumes:
        - configMap:
            name: {{ include "spilo-art.fullname" . }}-backup
          name: config
        - persistentVolumeClaim:
            {{- if empty .Values.backup.externalPvcName }}
            claimName: {{ include "spilo-art.fullname" . }}-backup
            {{- else }}
            claimName: {{ .Values.backup.externalPvcName }}
            {{- end }}
          name: backup
      {{- end }}
  volumeClaimTemplates:
  - metadata:
      labels:
        {{- include "spilo-art.selectorLabels" . | nindent 8 }}
      name: pgdata
    spec:
      {{- if .Values.data.storageClassName }}
      storageClassName: {{ .Values.data.storageClassName }}
      {{- end }}
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: {{ .Values.data.storage }}
#  - metadata:
#      labels:
#        application: spilo
#        spilo-cluster: *cluster_name
#      name: backup
#    spec:
#      storageClassName: managed-nfs-storage
#      accessModes:
#        - ReadWriteOnce
#      resources:
#        requests:
#          storage: 2Gi